{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n"
      ],
      "metadata": {
        "id": "PTcSI8nfZD29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTh6oStSvEkW",
        "outputId": "f571333d-bd1e-49f4-d964-5507691438e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnetwKaRwKEk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torchvision.utils import save_image\n",
        "#import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torchvision.transforms.functional as fn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHQze6Z3DNgp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-oESaZqDNm9"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgWCBT_JDPro",
        "outputId": "dd979493-824b-4c19-995a-a6f06bb0a804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images in class 1: 441\n",
            "images in class 0: 191\n"
          ]
        }
      ],
      "source": [
        "#dataset_path = '/content/gdrive/MyDrive/NeedleImages/'\n",
        "labels_path = '/content/gdrive/MyDrive/NeedleImages/Labelsc.csv'\n",
        "img_labels = pd.read_csv(labels_path)\n",
        "labels = pd.Series(np.where(img_labels['Label'] == 'yes', 1, 0),img_labels.index)\n",
        "#print(labels)\n",
        "print(\"images in class 1:\", labels.value_counts()[1])\n",
        "print(\"images in class 0:\", labels.value_counts()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNI6xL9U9HXQ"
      },
      "source": [
        "#Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm-KEoUc8xuc"
      },
      "outputs": [],
      "source": [
        "# Dataloader\n",
        "\n",
        "class MedDataset_new(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        # print(self.img_labels.shape)\n",
        "        # print(self.img_labels)\n",
        "\n",
        "        self.img_labels['Label'] = pd.Series(np.where(self.img_labels['Label'] == 'yes', 1, 0),\n",
        "          self.img_labels.index)\n",
        "       # self.im\n",
        "        # print(self.img_labels)\n",
        "        #print( self.img_labels.iloc[0, 2])\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #img_path = os.path.join(self.img_dir, ('n'+self.img_labels.iloc[idx, 1]))\n",
        "        img_path = os.path.join(self.img_dir, (self.img_labels.iloc[idx, 1]))\n",
        "        image = read_image(img_path)\n",
        "\n",
        "       # image  = torch.cat((image, image, image), 0)\n",
        "        #print(image.shape)\n",
        "        # if resize:\n",
        "        image = fn.resize(image,128)\n",
        "        label = self.img_labels.iloc[idx, 2]\n",
        "\n",
        "        #add contrast\n",
        "        #image = fn.adjust_contrast(image, 6)\n",
        "        # print(label )\n",
        "\n",
        "        #print(image.shape)\n",
        "        #plt.imshow(image[0])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        # plt.imshow(image[0])\n",
        "        # print(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyBWFYALyZxW"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/gdrive/MyDrive/NeedleImages/'\n",
        "\n",
        "#dataset_path = '/content/gdrive/MyDrive/NeedleImages/imgs1/'\n",
        "# dataset = MedDataset(os.path.join(dataset_path,'Labels.csv'), dataset_path)\n",
        "dataset = MedDataset_new('/content/gdrive/MyDrive/NeedleImages/Labelsc.csv', dataset_path)\n",
        "generator1 = torch.Generator().manual_seed(22)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [505,127])\n",
        "\n",
        "trainloader = DataLoader(dataset = train_set, batch_size = 64, shuffle = True, generator=generator1)\n",
        "testloader = DataLoader(dataset= test_set, batch_size = 64, shuffle = True, generator=generator1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DeepCNN"
      ],
      "metadata": {
        "id": "sCKzg_plQNV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepCNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 576, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = X.view(X.shape[0], -1)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "-MGpZVv5QOQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepCNN()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VukgUeKRBTBz",
        "outputId": "d6195c07-e697-4336-8ec6-3af090a5aed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 2.4305, train acc: 0.5728, time: 2.8549582958221436\n",
            "Epoch: 0, test loss: 0.6872, test acc: 0.5200, time: 3.3820252418518066\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6368, train acc: 0.6527, time: 6.226945877075195\n",
            "Epoch: 1, test loss: 0.5803, test acc: 0.7329, time: 6.774516820907593\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.5821, train acc: 0.6859, time: 9.665087699890137\n",
            "Epoch: 2, test loss: 0.5362, test acc: 0.7475, time: 10.22899866104126\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.5481, train acc: 0.7140, time: 14.116719007492065\n",
            "Epoch: 3, test loss: 0.5797, test acc: 0.6615, time: 14.777974367141724\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.5236, train acc: 0.7393, time: 17.667240381240845\n",
            "Epoch: 4, test loss: 0.5433, test acc: 0.6928, time: 18.19721269607544\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.4632, train acc: 0.7625, time: 21.022440671920776\n",
            "Epoch: 5, test loss: 0.5368, test acc: 0.6772, time: 21.574347734451294\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.3992, train acc: 0.8243, time: 24.37673544883728\n",
            "Epoch: 6, test loss: 0.4888, test acc: 0.7483, time: 24.982797384262085\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.3204, train acc: 0.8640, time: 28.946208477020264\n",
            "Epoch: 7, test loss: 0.5094, test acc: 0.7243, time: 29.51049828529358\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.2636, train acc: 0.9016, time: 32.40940880775452\n",
            "Epoch: 8, test loss: 0.5066, test acc: 0.7561, time: 32.951746702194214\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.1544, train acc: 0.9566, time: 35.94591784477234\n",
            "Epoch: 9, test loss: 0.5771, test acc: 0.6932, time: 36.468303203582764\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.0919, train acc: 0.9761, time: 39.74861121177673\n",
            "Epoch: 10, test loss: 0.6368, test acc: 0.7484, time: 40.49138426780701\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.0398, train acc: 0.9922, time: 44.341962575912476\n",
            "Epoch: 11, test loss: 0.6584, test acc: 0.7483, time: 44.88385725021362\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.0177, train acc: 1.0000, time: 48.017181634902954\n",
            "Epoch: 12, test loss: 0.7640, test acc: 0.7168, time: 48.59024739265442\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.0104, train acc: 1.0000, time: 51.66681385040283\n",
            "Epoch: 13, test loss: 0.8445, test acc: 0.7164, time: 52.20892310142517\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.0076, train acc: 1.0000, time: 55.94588541984558\n",
            "Epoch: 14, test loss: 0.8831, test acc: 0.7321, time: 56.66836643218994\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.0033, train acc: 1.0000, time: 60.12307548522949\n",
            "Epoch: 15, test loss: 0.9524, test acc: 0.7403, time: 60.689162492752075\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.0016, train acc: 1.0000, time: 63.76923942565918\n",
            "Epoch: 16, test loss: 0.8534, test acc: 0.7396, time: 64.34393906593323\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.0011, train acc: 1.0000, time: 67.41343998908997\n",
            "Epoch: 17, test loss: 0.9042, test acc: 0.7323, time: 68.00433683395386\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.0007, train acc: 1.0000, time: 72.25277829170227\n",
            "Epoch: 18, test loss: 0.9247, test acc: 0.7481, time: 72.83765983581543\n",
            "\n",
            "....."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-85f8b38e3d7f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0b29d68dc8b4>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#img_path = os.path.join(self.img_dir, ('n'+self.img_labels.iloc[idx, 1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m        \u001b[0;31m# image  = torch.cat((image, image, image), 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "oEs5DtvbIumg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 14400, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        # X = F.relu(self.conv2(X))\n",
        "        # X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = X.view(X.shape[0], -1)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "x5kTs33NRkV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PC3S_IF2I60z",
        "outputId": "6b874305-9a09-419a-83fa-ce907ce064b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 67.9368, train acc: 0.5977, time: 8.157029390335083\n",
            "Epoch: 0, test loss: 1.7022, test acc: 0.3074, time: 8.809293985366821\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 2.4448, train acc: 0.5726, time: 13.477833032608032\n",
            "Epoch: 1, test loss: 0.4954, test acc: 0.7561, time: 14.317841053009033\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.8982, train acc: 0.6314, time: 20.09014630317688\n",
            "Epoch: 2, test loss: 0.7579, test acc: 0.7323, time: 20.93763780593872\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.5734, train acc: 0.7266, time: 25.062905311584473\n",
            "Epoch: 3, test loss: 0.4675, test acc: 0.7796, time: 25.68881058692932\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.4335, train acc: 0.7789, time: 30.980095863342285\n",
            "Epoch: 4, test loss: 0.4730, test acc: 0.7638, time: 31.686641931533813\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.3843, train acc: 0.8509, time: 35.67477369308472\n",
            "Epoch: 5, test loss: 0.4872, test acc: 0.7715, time: 36.322324991226196\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.3223, train acc: 0.8621, time: 40.3416051864624\n",
            "Epoch: 6, test loss: 0.4675, test acc: 0.7558, time: 40.98472499847412\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.2801, train acc: 0.9182, time: 46.3832483291626\n",
            "Epoch: 7, test loss: 0.4850, test acc: 0.7791, time: 47.05643558502197\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.2331, train acc: 0.9294, time: 51.30206036567688\n",
            "Epoch: 8, test loss: 0.4532, test acc: 0.7800, time: 51.97223114967346\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.1934, train acc: 0.9529, time: 56.4039101600647\n",
            "Epoch: 9, test loss: 0.4830, test acc: 0.7799, time: 57.30231475830078\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.1501, train acc: 0.9741, time: 62.7049674987793\n",
            "Epoch: 10, test loss: 0.5125, test acc: 0.7875, time: 63.66982936859131\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.1085, train acc: 0.9902, time: 67.95305466651917\n",
            "Epoch: 11, test loss: 0.5172, test acc: 0.7639, time: 68.6071264743805\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.0787, train acc: 0.9919, time: 73.8680431842804\n",
            "Epoch: 12, test loss: 0.5320, test acc: 0.7796, time: 74.72885417938232\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.0550, train acc: 0.9941, time: 78.8636622428894\n",
            "Epoch: 13, test loss: 0.5768, test acc: 0.7713, time: 79.49385905265808\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.0380, train acc: 0.9980, time: 83.53222012519836\n",
            "Epoch: 14, test loss: 0.6203, test acc: 0.7716, time: 84.18071389198303\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.0267, train acc: 0.9980, time: 89.52190351486206\n",
            "Epoch: 15, test loss: 0.6400, test acc: 0.7873, time: 90.15169477462769\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.0195, train acc: 1.0000, time: 94.3098497390747\n",
            "Epoch: 16, test loss: 0.6876, test acc: 0.7872, time: 94.95665788650513\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.0143, train acc: 1.0000, time: 99.03403806686401\n",
            "Epoch: 17, test loss: 0.6993, test acc: 0.7878, time: 99.81697869300842\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.0112, train acc: 1.0000, time: 105.11392068862915\n",
            "Epoch: 18, test loss: 0.7571, test acc: 0.7638, time: 105.79436421394348\n",
            "\n",
            "........\n",
            " --- Epoch: 19, train loss: 0.0091, train acc: 1.0000, time: 110.2090802192688\n",
            "Epoch: 19, test loss: 0.7491, test acc: 0.7958, time: 110.92628645896912\n",
            "\n",
            "........\n",
            " --- Epoch: 20, train loss: 0.0070, train acc: 1.0000, time: 115.63392972946167\n",
            "Epoch: 20, test loss: 0.7892, test acc: 0.7716, time: 116.48413348197937\n",
            "\n",
            "........\n",
            " --- Epoch: 21, train loss: 0.0057, train acc: 1.0000, time: 121.10888433456421\n",
            "Epoch: 21, test loss: 0.7987, test acc: 0.7791, time: 121.76489973068237\n",
            "\n",
            "........\n",
            " --- Epoch: 22, train loss: 0.0048, train acc: 1.0000, time: 126.00759625434875\n",
            "Epoch: 22, test loss: 0.8165, test acc: 0.7878, time: 126.69061207771301\n",
            "\n",
            "........\n",
            " --- Epoch: 23, train loss: 0.0040, train acc: 1.0000, time: 131.8528594970703\n",
            "Epoch: 23, test loss: 0.8358, test acc: 0.7873, time: 132.69062399864197\n",
            "\n",
            "........\n",
            " --- Epoch: 24, train loss: 0.0035, train acc: 1.0000, time: 136.7718529701233\n",
            "Epoch: 24, test loss: 0.8510, test acc: 0.7877, time: 137.4255747795105\n",
            "\n",
            "........\n",
            " --- Epoch: 25, train loss: 0.0031, train acc: 1.0000, time: 141.55923295021057\n",
            "Epoch: 25, test loss: 0.8742, test acc: 0.7790, time: 142.21241211891174\n",
            "\n",
            "........\n",
            " --- Epoch: 26, train loss: 0.0027, train acc: 1.0000, time: 147.58967113494873\n",
            "Epoch: 26, test loss: 0.8794, test acc: 0.7877, time: 148.22842121124268\n",
            "\n",
            "........\n",
            " --- Epoch: 27, train loss: 0.0024, train acc: 1.0000, time: 152.4267542362213\n",
            "Epoch: 27, test loss: 0.8957, test acc: 0.7870, time: 153.0491383075714\n",
            "\n",
            "."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-dd68acd09281>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-81766bcda50f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preds.argmax(dim=1), y)\n",
        "print(test_preds.argmax(dim=1), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDddRO1EI8__",
        "outputId": "30cd4099-599b-4a27-e249-2cc35451fe1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1]) tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1])\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1]) tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = 0\n",
        "c2 = 0\n",
        "print(len(y))\n",
        "for i in range(len(test_y)):\n",
        "\n",
        "  if test_preds.argmax(dim=1)[i]== test_y[i]:\n",
        "    c1 = c1+1\n",
        "  else:\n",
        "    print(preds.argmax(dim=1)[i], y[i])\n",
        "    c2 = c2+1\n",
        "print(c1,c2)\n",
        "accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "print(accuracy)\n",
        "#len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-hGA_OrJ2-J",
        "outputId": "7afed8b2-67ff-4a5b-b243-b37fb43eb80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tensor(1) tensor(0)\n",
            "tensor(1) tensor(0)\n",
            "tensor(1) tensor(0)\n",
            "tensor(0) tensor(1)\n",
            "tensor(1) tensor(1)\n",
            "tensor(0) tensor(1)\n",
            "tensor(1) tensor(0)\n",
            "tensor(0) tensor(0)\n",
            "tensor(1) tensor(0)\n",
            "tensor(1) tensor(1)\n",
            "tensor(0) tensor(1)\n",
            "tensor(0) tensor(0)\n",
            "tensor(1) tensor(0)\n",
            "tensor(0) tensor(1)\n",
            "tensor(1) tensor(0)\n",
            "tensor(0) tensor(1)\n",
            "tensor(0) tensor(0)\n",
            "46 17\n",
            "tensor(0.7302)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#arch3: Alexnet"
      ],
      "metadata": {
        "id": "KyWRL1pPRWVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8E6cA_hWKWVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWIn2NWURoTW",
        "outputId": "cfc0170f-7cd0-4c7b-8328-4a2908cb734b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 3.4500, train acc: 0.5294, time: 24.726656675338745\n",
            "Epoch: 0, test loss: 0.6107, test acc: 0.7319, time: 26.583229541778564\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6246, train acc: 0.6895, time: 48.54720687866211\n",
            "Epoch: 1, test loss: 0.5584, test acc: 0.7326, time: 51.42438292503357\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.5844, train acc: 0.6895, time: 73.0847795009613\n",
            "Epoch: 2, test loss: 0.5435, test acc: 0.7320, time: 74.8574652671814\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.5649, train acc: 0.6898, time: 98.0824248790741\n",
            "Epoch: 3, test loss: 0.5443, test acc: 0.7320, time: 99.86289930343628\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.5351, train acc: 0.6883, time: 122.41669464111328\n",
            "Epoch: 4, test loss: 0.5282, test acc: 0.7323, time: 125.28435754776001\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.5299, train acc: 0.6956, time: 147.34710097312927\n",
            "Epoch: 5, test loss: 0.5458, test acc: 0.7242, time: 149.25779914855957\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.5116, train acc: 0.6847, time: 172.8148455619812\n",
            "Epoch: 6, test loss: 0.5535, test acc: 0.6301, time: 174.58169388771057\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.4761, train acc: 0.7323, time: 199.0335657596588\n",
            "Epoch: 7, test loss: 0.5514, test acc: 0.6616, time: 201.69829559326172\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.4690, train acc: 0.7469, time: 222.97746896743774\n",
            "Epoch: 8, test loss: 0.5940, test acc: 0.6375, time: 224.7894585132599\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.4663, train acc: 0.7442, time: 248.1682255268097\n",
            "Epoch: 9, test loss: 0.6303, test acc: 0.6610, time: 249.96704697608948\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.4119, train acc: 0.8125, time: 272.54911947250366\n",
            "Epoch: 10, test loss: 0.5988, test acc: 0.7324, time: 275.36721563339233\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.3388, train acc: 0.8457, time: 297.99137711524963\n",
            "Epoch: 11, test loss: 0.9103, test acc: 0.7324, time: 299.871417760849\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.3744, train acc: 0.8331, time: 325.71050691604614\n",
            "Epoch: 12, test loss: 0.6374, test acc: 0.6218, time: 327.65755915641785\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.3600, train acc: 0.8313, time: 354.2592875957489\n",
            "Epoch: 13, test loss: 0.6385, test acc: 0.7166, time: 356.1692781448364\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.2921, train acc: 0.8702, time: 381.3065311908722\n",
            "Epoch: 14, test loss: 0.7971, test acc: 0.7087, time: 383.1732954978943\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.2337, train acc: 0.9070, time: 410.056027173996\n",
            "Epoch: 15, test loss: 0.8201, test acc: 0.6850, time: 411.8815321922302\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.1624, train acc: 0.9326, time: 435.42518424987793\n",
            "Epoch: 16, test loss: 1.0202, test acc: 0.7163, time: 438.2771987915039\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.1775, train acc: 0.9170, time: 461.8118088245392\n",
            "Epoch: 17, test loss: 1.3408, test acc: 0.7004, time: 463.67103147506714\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.1294, train acc: 0.9409, time: 488.1521942615509\n",
            "Epoch: 18, test loss: 1.5485, test acc: 0.6775, time: 489.9691526889801\n",
            "\n",
            "........\n",
            " --- Epoch: 19, train loss: 0.1289, train acc: 0.9622, time: 514.5660660266876\n",
            "Epoch: 19, test loss: 0.9968, test acc: 0.6931, time: 516.365496635437\n",
            "\n",
            "........\n",
            " --- Epoch: 20, train loss: 0.0735, train acc: 0.9688, time: 540.6506798267365\n",
            "Epoch: 20, test loss: 1.1169, test acc: 0.7091, time: 542.8012068271637\n",
            "\n",
            "........\n",
            " --- Epoch: 21, train loss: 0.0411, train acc: 0.9863, time: 565.5517761707306\n",
            "Epoch: 21, test loss: 1.8844, test acc: 0.7405, time: 567.9548056125641\n",
            "\n",
            "........\n",
            " --- Epoch: 22, train loss: 0.0836, train acc: 0.9739, time: 591.9573583602905\n",
            "Epoch: 22, test loss: 1.4632, test acc: 0.7238, time: 594.238067150116\n",
            "\n",
            "........\n",
            " --- Epoch: 23, train loss: 0.0617, train acc: 0.9800, time: 620.9306907653809\n",
            "Epoch: 23, test loss: 1.1277, test acc: 0.7325, time: 622.8096358776093\n",
            "\n",
            "........\n",
            " --- Epoch: 24, train loss: 0.0488, train acc: 0.9839, time: 648.0052285194397\n",
            "Epoch: 24, test loss: 1.6059, test acc: 0.7005, time: 649.8275098800659\n",
            "\n",
            "........\n",
            " --- Epoch: 25, train loss: 0.0469, train acc: 0.9844, time: 673.5274622440338\n",
            "Epoch: 25, test loss: 2.6933, test acc: 0.7087, time: 676.2399880886078\n",
            "\n",
            "........\n",
            " --- Epoch: 26, train loss: 0.1978, train acc: 0.9341, time: 698.8811285495758\n",
            "Epoch: 26, test loss: 0.8239, test acc: 0.7560, time: 700.7051050662994\n",
            "\n",
            "........\n",
            " --- Epoch: 27, train loss: 0.1167, train acc: 0.9392, time: 725.168497800827\n",
            "Epoch: 27, test loss: 1.3147, test acc: 0.6767, time: 726.9998393058777\n",
            "\n",
            "........\n",
            " --- Epoch: 28, train loss: 0.0317, train acc: 0.9863, time: 751.6934762001038\n",
            "Epoch: 28, test loss: 1.7550, test acc: 0.7476, time: 753.509889125824\n",
            "\n",
            "........\n",
            " --- Epoch: 29, train loss: 0.0397, train acc: 0.9824, time: 779.4549930095673\n",
            "Epoch: 29, test loss: 2.1438, test acc: 0.7088, time: 781.3599677085876\n",
            "\n",
            "........\n",
            " --- Epoch: 30, train loss: 0.0664, train acc: 0.9841, time: 808.3523414134979\n",
            "Epoch: 30, test loss: 2.2209, test acc: 0.6611, time: 810.3223638534546\n",
            "\n",
            "........\n",
            " --- Epoch: 31, train loss: 0.1644, train acc: 0.9470, time: 833.8775203227997\n",
            "Epoch: 31, test loss: 0.8995, test acc: 0.7638, time: 836.6358218193054\n",
            "\n",
            "........\n",
            " --- Epoch: 32, train loss: 0.0907, train acc: 0.9668, time: 864.7684667110443\n",
            "Epoch: 32, test loss: 1.3940, test acc: 0.7010, time: 867.7005865573883\n",
            "\n",
            "...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58iFhRFwRu-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#arch4 : alexnet modified"
      ],
      "metadata": {
        "id": "lc5xEVwt-Nkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        # out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        #out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gaNC2Y2k-Q0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU48-mhF-V0_",
        "outputId": "5fad6a98-f60f-4e87-ad39-77fce1f672f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 2.6843, train acc: 0.5399, time: 215.02845931053162\n",
            "Epoch: 0, test loss: 0.6588, test acc: 0.7326, time: 272.28511118888855\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6787, train acc: 0.6900, time: 291.14488911628723\n",
            "Epoch: 1, test loss: 0.5909, test acc: 0.7324, time: 293.2492868900299\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.6259, train acc: 0.6888, time: 310.86167430877686\n",
            "Epoch: 2, test loss: 0.5773, test acc: 0.7325, time: 312.5045199394226\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.5912, train acc: 0.6905, time: 330.15379905700684\n",
            "Epoch: 3, test loss: 0.5265, test acc: 0.7319, time: 331.6910607814789\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.5954, train acc: 0.6886, time: 351.28659105300903\n",
            "Epoch: 4, test loss: 0.5270, test acc: 0.7316, time: 353.69235587120056\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.5866, train acc: 0.6851, time: 371.9740309715271\n",
            "Epoch: 5, test loss: 0.5350, test acc: 0.7476, time: 373.617463350296\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.5675, train acc: 0.6969, time: 390.9176824092865\n",
            "Epoch: 6, test loss: 0.5053, test acc: 0.7562, time: 392.5443937778473\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.5543, train acc: 0.6856, time: 411.84943103790283\n",
            "Epoch: 7, test loss: 0.5004, test acc: 0.7795, time: 413.32190108299255\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.5368, train acc: 0.7166, time: 430.7205753326416\n",
            "Epoch: 8, test loss: 0.4951, test acc: 0.7396, time: 432.20579743385315\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.5129, train acc: 0.7342, time: 449.359694480896\n",
            "Epoch: 9, test loss: 0.4969, test acc: 0.7557, time: 450.78319454193115\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.4916, train acc: 0.7493, time: 470.1599311828613\n",
            "Epoch: 10, test loss: 0.4949, test acc: 0.7243, time: 471.89358854293823\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.4298, train acc: 0.7894, time: 489.3738217353821\n",
            "Epoch: 11, test loss: 0.5066, test acc: 0.7636, time: 490.8826370239258\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.3852, train acc: 0.8238, time: 508.5062940120697\n",
            "Epoch: 12, test loss: 0.6342, test acc: 0.6846, time: 510.1346492767334\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.3449, train acc: 0.8264, time: 529.5522742271423\n",
            "Epoch: 13, test loss: 0.5545, test acc: 0.7717, time: 532.0033259391785\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.2133, train acc: 0.9146, time: 550.3510522842407\n",
            "Epoch: 14, test loss: 0.7701, test acc: 0.7009, time: 551.8193459510803\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.1327, train acc: 0.9505, time: 569.6958391666412\n",
            "Epoch: 15, test loss: 0.7887, test acc: 0.7408, time: 571.7429184913635\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.1421, train acc: 0.9380, time: 590.9070055484772\n",
            "Epoch: 16, test loss: 1.1058, test acc: 0.7087, time: 592.4335672855377\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.1569, train acc: 0.9380, time: 610.2364401817322\n",
            "Epoch: 17, test loss: 0.8163, test acc: 0.7246, time: 611.8146555423737\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.0807, train acc: 0.9705, time: 629.7521567344666\n",
            "Epoch: 18, test loss: 1.1075, test acc: 0.7480, time: 632.0940580368042\n",
            "\n",
            "........\n",
            " --- Epoch: 19, train loss: 0.0747, train acc: 0.9668, time: 650.949184179306\n",
            "Epoch: 19, test loss: 1.1784, test acc: 0.7481, time: 652.4997887611389\n",
            "\n",
            "........\n",
            " --- Epoch: 20, train loss: 0.0621, train acc: 0.9783, time: 672.7655575275421\n",
            "Epoch: 20, test loss: 1.2555, test acc: 0.7796, time: 674.3378059864044\n",
            "\n",
            "........\n",
            " --- Epoch: 21, train loss: 0.1380, train acc: 0.9532, time: 694.5976593494415\n",
            "Epoch: 21, test loss: 1.0814, test acc: 0.7636, time: 696.1319687366486\n",
            "\n",
            "........\n",
            " --- Epoch: 22, train loss: 0.1169, train acc: 0.9580, time: 718.0723493099213\n",
            "Epoch: 22, test loss: 1.1336, test acc: 0.6923, time: 719.686487197876\n",
            "\n",
            "........\n",
            " --- Epoch: 23, train loss: 0.0771, train acc: 0.9683, time: 738.7014713287354\n",
            "Epoch: 23, test loss: 0.7210, test acc: 0.7479, time: 740.5053412914276\n",
            "\n",
            "........\n",
            " --- Epoch: 24, train loss: 0.0801, train acc: 0.9724, time: 760.499434709549\n",
            "Epoch: 24, test loss: 1.0355, test acc: 0.7400, time: 761.9718291759491\n",
            "\n",
            "........\n",
            " --- Epoch: 25, train loss: 0.0642, train acc: 0.9790, time: 780.8244519233704\n",
            "Epoch: 25, test loss: 0.9475, test acc: 0.7716, time: 782.3530440330505\n",
            "\n",
            "........\n",
            " --- Epoch: 26, train loss: 0.0253, train acc: 0.9922, time: 800.1484098434448\n",
            "Epoch: 26, test loss: 1.0529, test acc: 0.7721, time: 802.4504528045654\n",
            "\n",
            "........\n",
            " --- Epoch: 27, train loss: 0.0204, train acc: 0.9941, time: 820.5239050388336\n",
            "Epoch: 27, test loss: 1.0708, test acc: 0.7557, time: 822.13822722435\n",
            "\n",
            "........\n",
            " --- Epoch: 28, train loss: 0.0082, train acc: 0.9980, time: 839.684291601181\n",
            "Epoch: 28, test loss: 1.1771, test acc: 0.7479, time: 841.2404458522797\n",
            "\n",
            "........\n",
            " --- Epoch: 29, train loss: 0.0043, train acc: 1.0000, time: 860.13858294487\n",
            "Epoch: 29, test loss: 1.2603, test acc: 0.7954, time: 862.5743639469147\n",
            "\n",
            "........\n",
            " --- Epoch: 30, train loss: 0.0019, train acc: 1.0000, time: 880.6924142837524\n",
            "Epoch: 30, test loss: 1.2325, test acc: 0.7798, time: 882.9272048473358\n",
            "\n",
            "........\n",
            " --- Epoch: 31, train loss: 0.0012, train acc: 1.0000, time: 906.3062827587128\n",
            "Epoch: 31, test loss: 1.3522, test acc: 0.7872, time: 908.0279014110565\n",
            "\n",
            "......"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdfTXN_r-wOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#arch5: alexnet modified"
      ],
      "metadata": {
        "id": "FvHz5_66K4CT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "biCn7fmmpMqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        #out = self.layer3(out)\n",
        "        # out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        #out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_uDpDr38K5zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw-PdQ6OLB7g",
        "outputId": "a9651e3a-8639-4f6f-d2e2-b9257551e0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 2.3157, train acc: 0.5389, time: 16.511329650878906\n",
            "Epoch: 0, test loss: 0.6209, test acc: 0.6687, time: 17.82980728149414\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6079, train acc: 0.7059, time: 33.89907622337341\n",
            "Epoch: 1, test loss: 0.6120, test acc: 0.6693, time: 35.51865267753601\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.5855, train acc: 0.7042, time: 53.77096724510193\n",
            "Epoch: 2, test loss: 0.6015, test acc: 0.6695, time: 55.45494985580444\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.5701, train acc: 0.7047, time: 70.64759516716003\n",
            "Epoch: 3, test loss: 0.6022, test acc: 0.6694, time: 71.96150708198547\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.5606, train acc: 0.7047, time: 87.63208818435669\n",
            "Epoch: 4, test loss: 0.5895, test acc: 0.6768, time: 89.05343317985535\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.5481, train acc: 0.7047, time: 104.35402488708496\n",
            "Epoch: 5, test loss: 0.6005, test acc: 0.7004, time: 105.78962850570679\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.5285, train acc: 0.7156, time: 120.90353560447693\n",
            "Epoch: 6, test loss: 0.5857, test acc: 0.7087, time: 122.28651428222656\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.5323, train acc: 0.7210, time: 137.48569130897522\n",
            "Epoch: 7, test loss: 0.5551, test acc: 0.7160, time: 139.04790449142456\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.5160, train acc: 0.7357, time: 155.59228324890137\n",
            "Epoch: 8, test loss: 0.5666, test acc: 0.7171, time: 157.68132138252258\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.4844, train acc: 0.7403, time: 173.1747281551361\n",
            "Epoch: 9, test loss: 0.5456, test acc: 0.6930, time: 174.53485202789307\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.4685, train acc: 0.7789, time: 189.416836977005\n",
            "Epoch: 10, test loss: 0.5535, test acc: 0.7324, time: 190.69523859024048\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.4469, train acc: 0.7679, time: 205.59976863861084\n",
            "Epoch: 11, test loss: 0.6263, test acc: 0.7242, time: 206.9331409931183\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.3980, train acc: 0.8001, time: 223.8812177181244\n",
            "Epoch: 12, test loss: 0.5458, test acc: 0.7328, time: 225.20962381362915\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.3748, train acc: 0.8062, time: 240.13967490196228\n",
            "Epoch: 13, test loss: 0.5487, test acc: 0.7245, time: 241.53848218917847\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.2861, train acc: 0.8819, time: 256.5064401626587\n",
            "Epoch: 14, test loss: 0.6160, test acc: 0.7401, time: 258.5578181743622\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.2310, train acc: 0.9038, time: 273.39226269721985\n",
            "Epoch: 15, test loss: 0.5513, test acc: 0.7326, time: 275.3247573375702\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.1612, train acc: 0.9339, time: 290.0145719051361\n",
            "Epoch: 16, test loss: 0.6215, test acc: 0.8033, time: 291.54071259498596\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.1385, train acc: 0.9432, time: 306.87641954421997\n",
            "Epoch: 17, test loss: 0.9969, test acc: 0.7401, time: 308.150027513504\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.1426, train acc: 0.9524, time: 323.15339708328247\n",
            "Epoch: 18, test loss: 0.9288, test acc: 0.7951, time: 324.522337436676\n",
            "\n",
            "........\n",
            " --- Epoch: 19, train loss: 0.1282, train acc: 0.9463, time: 339.89871430397034\n",
            "Epoch: 19, test loss: 0.9075, test acc: 0.7320, time: 341.18586778640747\n",
            "\n",
            "........\n",
            " --- Epoch: 20, train loss: 0.1377, train acc: 0.9419, time: 356.49404740333557\n",
            "Epoch: 20, test loss: 0.7941, test acc: 0.7715, time: 357.9132385253906\n",
            "\n",
            "........\n",
            " --- Epoch: 21, train loss: 0.1347, train acc: 0.9451, time: 373.42477440834045\n",
            "Epoch: 21, test loss: 1.0462, test acc: 0.7791, time: 375.01416015625\n",
            "\n",
            "........\n",
            " --- Epoch: 22, train loss: 0.0844, train acc: 0.9680, time: 392.5098805427551\n",
            "Epoch: 22, test loss: 1.3992, test acc: 0.7405, time: 394.43203115463257\n",
            "\n",
            "........\n",
            " --- Epoch: 23, train loss: 0.0725, train acc: 0.9741, time: 413.6167137622833\n",
            "Epoch: 23, test loss: 1.1047, test acc: 0.7474, time: 414.92481780052185\n",
            "\n",
            "........\n",
            " --- Epoch: 24, train loss: 0.0845, train acc: 0.9622, time: 430.3107039928436\n",
            "Epoch: 24, test loss: 0.8092, test acc: 0.7561, time: 431.6830222606659\n",
            "\n",
            "........\n",
            " --- Epoch: 25, train loss: 0.0611, train acc: 0.9763, time: 447.2445924282074\n",
            "Epoch: 25, test loss: 1.2647, test acc: 0.7324, time: 448.569007396698\n",
            "\n",
            "........\n",
            " --- Epoch: 26, train loss: 0.0227, train acc: 0.9941, time: 464.12577652931213\n",
            "Epoch: 26, test loss: 1.0742, test acc: 0.7397, time: 466.1310420036316\n",
            "\n",
            "........\n",
            " --- Epoch: 27, train loss: 0.0322, train acc: 0.9902, time: 481.2633366584778\n",
            "Epoch: 27, test loss: 1.3208, test acc: 0.7248, time: 483.1798951625824\n",
            "\n",
            "........\n",
            " --- Epoch: 28, train loss: 0.0338, train acc: 0.9844, time: 498.28231978416443\n",
            "Epoch: 28, test loss: 1.3033, test acc: 0.7480, time: 499.6204857826233\n",
            "\n",
            "........\n",
            " --- Epoch: 29, train loss: 0.0366, train acc: 0.9883, time: 515.020542383194\n",
            "Epoch: 29, test loss: 1.7220, test acc: 0.6933, time: 516.316547870636\n",
            "\n",
            "........\n",
            " --- Epoch: 30, train loss: 0.0163, train acc: 0.9922, time: 534.4491982460022\n",
            "Epoch: 30, test loss: 1.1207, test acc: 0.7406, time: 535.7972791194916\n",
            "\n",
            "........\n",
            " --- Epoch: 31, train loss: 0.0073, train acc: 0.9978, time: 552.696718454361\n",
            "Epoch: 31, test loss: 1.2108, test acc: 0.7243, time: 554.3195176124573\n",
            "\n",
            "........\n",
            " --- Epoch: 32, train loss: 0.0089, train acc: 0.9980, time: 570.1994118690491\n",
            "Epoch: 32, test loss: 1.6085, test acc: 0.7564, time: 572.6271634101868\n",
            "\n",
            "........\n",
            " --- Epoch: 33, train loss: 0.0074, train acc: 0.9978, time: 600.8025476932526\n",
            "Epoch: 33, test loss: 1.7627, test acc: 0.7243, time: 602.3450560569763\n",
            "\n",
            ".."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjGvu1b5LEUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resnet"
      ],
      "metadata": {
        "id": "bAj_H3vcpOYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "\n",
        "class MedDataset_new(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        # print(self.img_labels.shape)\n",
        "        # print(self.img_labels)\n",
        "\n",
        "        self.img_labels['Label'] = pd.Series(np.where(self.img_labels['Label'] == 'yes', 1, 0),\n",
        "          self.img_labels.index)\n",
        "       # self.im\n",
        "        # print(self.img_labels)\n",
        "        #print( self.img_labels.iloc[0, 2])\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, ('n'+self.img_labels.iloc[idx, 1]))\n",
        "        #img_path = os.path.join(self.img_dir, (self.img_labels.iloc[idx, 1]))\n",
        "        image = read_image(img_path)\n",
        "\n",
        "       # image  = torch.cat((image, image, image), 0)\n",
        "        #print(image.shape)\n",
        "        # if resize:\n",
        "        image = fn.resize(image,128)\n",
        "        label = self.img_labels.iloc[idx, 2]\n",
        "\n",
        "        #add contrast\n",
        "        #image = fn.adjust_contrast(image, 6)\n",
        "        # print(label )\n",
        "\n",
        "        #print(image.shape)\n",
        "        #plt.imshow(image[0])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        # plt.imshow(image[0])\n",
        "        # print(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "l26VDvgipVs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset_path = '/content/gdrive/MyDrive/NeedleImages/'\n",
        "\n",
        "dataset_path = '/content/gdrive/MyDrive/NeedleImages/imgs1/'\n",
        "# dataset = MedDataset(os.path.join(dataset_path,'Labels.csv'), dataset_path)\n",
        "dataset = MedDataset_new('/content/gdrive/MyDrive/NeedleImages/Labelsc.csv', dataset_path)\n",
        "generator1 = torch.Generator().manual_seed(22)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [505,127])\n",
        "\n",
        "trainloader = DataLoader(dataset = train_set, batch_size = 64, shuffle = True, generator=generator1)\n",
        "testloader = DataLoader(dataset= test_set, batch_size = 64, shuffle = True, generator=generator1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9UlZX2ttpdKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNetBinaryClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetBinaryClassifier, self).__init__()\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3mdZBL8MpR1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetBinaryClassifier(2)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZpuphQep_R9",
        "outputId": "d9afb148-235d-4cce-ee16-0752c048661d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 0.8183, train acc: 0.6627, time: 342.83852887153625\n",
            "Epoch: 0, test loss: 0.8433, test acc: 0.6215, time: 458.99147486686707\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6198, train acc: 0.7601, time: 588.4147820472717\n",
            "Epoch: 1, test loss: 0.4630, test acc: 0.7407, time: 600.9250376224518\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.3258, train acc: 0.8484, time: 734.5288553237915\n",
            "Epoch: 2, test loss: 0.6019, test acc: 0.7558, time: 746.3216066360474\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.1661, train acc: 0.9351, time: 888.0674004554749\n",
            "Epoch: 3, test loss: 0.6097, test acc: 0.7795, time: 900.9041900634766\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNetBinaryClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetBinaryClassifier, self).__init__()\n",
        "        self.resnet = resnet50(pretrained=False)\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-i-80L_dpb2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetBinaryClassifier(2)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55efukHEOV1l",
        "outputId": "d089e423-62c2-4860-b77a-f6c6d4d27638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 1.1056, train acc: 0.5753, time: 302.94943404197693\n",
            "Epoch: 0, test loss: 0.6258, test acc: 0.6927, time: 355.4992220401764\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6877, train acc: 0.6605, time: 503.01544737815857\n",
            "Epoch: 1, test loss: 0.5364, test acc: 0.7083, time: 516.3734545707703\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.5306, train acc: 0.7525, time: 663.6416437625885\n",
            "Epoch: 2, test loss: 0.5459, test acc: 0.7480, time: 676.6102097034454\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.3618, train acc: 0.8382, time: 841.5610182285309\n",
            "Epoch: 3, test loss: 0.5435, test acc: 0.7715, time: 855.1657197475433\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN+Attention"
      ],
      "metadata": {
        "id": "MFAbnzKCchEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AttentionNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_heads, num_classes):\n",
        "        super(AttentionNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=(5, 5), )\n",
        "        self.attention = nn.MultiheadAttention(15376, num_heads)\n",
        "        self.fc = nn.Linear(16, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape the input image\n",
        "        #x = x.view(x.size(0), x.size(1), 128*128)  # (batch_size, input_dim, num_pixels)\n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = x.view(x.size(0), x.size(1), -1)\n",
        "        # Apply multi-head attention\n",
        "        x, _ = self.attention(x, x, x)\n",
        "\n",
        "        # Average pooling along the sequence dimension\n",
        "        x = torch.mean(x, dim=2)\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        # Apply non-linearity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Apply output layer\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "TH0EQSzNJFzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionNetwork(1,16, 8,2)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLqZFQ_LYxKT",
        "outputId": "29d29749-0549-4fdc-c087-20c9547273ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 16, 124, 124])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Transformer\n",
        "import torch.nn as nn\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 14400, out_features=512)\n",
        "        # self.fc2 = nn.Linear(in_features=512, out_features=50)\n",
        "        # self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "           # nn.Linear(256 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        X = F.relu(self.conv1(x))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        # X = F.relu(self.conv2(X))\n",
        "        # X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = X.view(X.shape[0], -1)\n",
        "        X = F.relu(self.fc1(X))\n",
        "         #print(X.shape)\n",
        "        X = self.attention(X)\n",
        "        # X = F.relu(self.fc2(X))\n",
        "\n",
        "        # X = self.fc3(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "1kSqxmNEFAAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mBpYJHRFTsa",
        "outputId": "8aa7683c-7db9-413e-c20c-eea82abfdc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 0.6381, train acc: 0.6678, time: 6.035770654678345\n",
            "Epoch: 0, test loss: 0.6518, test acc: 0.6615, time: 6.883571624755859\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6059, train acc: 0.7073, time: 11.843963146209717\n",
            "Epoch: 1, test loss: 0.6517, test acc: 0.6616, time: 12.468335628509521\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.6066, train acc: 0.7066, time: 16.647462844848633\n",
            "Epoch: 2, test loss: 0.6509, test acc: 0.6623, time: 17.26731562614441\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.6071, train acc: 0.7061, time: 21.858338594436646\n",
            "Epoch: 3, test loss: 0.6516, test acc: 0.6617, time: 22.6569504737854\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.6064, train acc: 0.7069, time: 27.211830615997314\n",
            "Epoch: 4, test loss: 0.6521, test acc: 0.6612, time: 27.825807809829712\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.6062, train acc: 0.7071, time: 31.890145778656006\n",
            "Epoch: 5, test loss: 0.6518, test acc: 0.6615, time: 32.49920153617859\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.6054, train acc: 0.7078, time: 37.39509725570679\n",
            "Epoch: 6, test loss: 0.6514, test acc: 0.6618, time: 38.24953103065491\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.6064, train acc: 0.7069, time: 42.52928304672241\n",
            "Epoch: 7, test loss: 0.6517, test acc: 0.6616, time: 43.14811110496521\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.6069, train acc: 0.7064, time: 47.2182092666626\n",
            "Epoch: 8, test loss: 0.6523, test acc: 0.6610, time: 47.824334383010864\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.6064, train acc: 0.7069, time: 53.09635019302368\n",
            "Epoch: 9, test loss: 0.6518, test acc: 0.6615, time: 53.82591652870178\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.6059, train acc: 0.7073, time: 57.892881870269775\n",
            "Epoch: 10, test loss: 0.6521, test acc: 0.6612, time: 58.5066933631897\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.6066, train acc: 0.7066, time: 62.54541325569153\n",
            "Epoch: 11, test loss: 0.6519, test acc: 0.6613, time: 63.167487144470215\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.6066, train acc: 0.7066, time: 68.58874011039734\n",
            "Epoch: 12, test loss: 0.6511, test acc: 0.6622, time: 69.19983625411987\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.6074, train acc: 0.7059, time: 73.24886417388916\n",
            "Epoch: 13, test loss: 0.6512, test acc: 0.6621, time: 73.87135624885559\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.6054, train acc: 0.7078, time: 77.956627368927\n",
            "Epoch: 14, test loss: 0.6523, test acc: 0.6610, time: 78.688068151474\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.6059, train acc: 0.7073, time: 83.95737075805664\n",
            "Epoch: 15, test loss: 0.6524, test acc: 0.6608, time: 84.56519842147827\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.6057, train acc: 0.7076, time: 88.61787462234497\n",
            "Epoch: 16, test loss: 0.6517, test acc: 0.6616, time: 89.2496120929718\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.6076, train acc: 0.7057, time: 93.4358913898468\n",
            "Epoch: 17, test loss: 0.6524, test acc: 0.6608, time: 94.26838970184326\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.6047, train acc: 0.7085, time: 99.23400712013245\n",
            "Epoch: 18, test loss: 0.6517, test acc: 0.6616, time: 99.86720418930054\n",
            "\n",
            "........\n",
            " --- Epoch: 19, train loss: 0.6050, train acc: 0.7083, time: 103.92943906784058\n",
            "Epoch: 19, test loss: 0.6518, test acc: 0.6615, time: 104.57131934165955\n",
            "\n",
            "........\n",
            " --- Epoch: 20, train loss: 0.6069, train acc: 0.7064, time: 109.12005853652954\n",
            "Epoch: 20, test loss: 0.6521, test acc: 0.6612, time: 109.91780805587769\n",
            "\n",
            "........\n",
            " --- Epoch: 21, train loss: 0.6066, train acc: 0.7066, time: 114.65730404853821\n",
            "Epoch: 21, test loss: 0.6523, test acc: 0.6610, time: 115.28330278396606\n",
            "\n",
            "........\n",
            " --- Epoch: 22, train loss: 0.6064, train acc: 0.7069, time: 119.34757733345032\n",
            "Epoch: 22, test loss: 0.6518, test acc: 0.6615, time: 119.96053695678711\n",
            "\n",
            "........\n",
            " --- Epoch: 23, train loss: 0.6064, train acc: 0.7069, time: 124.75964426994324\n",
            "Epoch: 23, test loss: 0.6513, test acc: 0.6620, time: 125.5801637172699\n",
            "\n",
            "........\n",
            " --- Epoch: 24, train loss: 0.6064, train acc: 0.7069, time: 129.97435855865479\n",
            "Epoch: 24, test loss: 0.6519, test acc: 0.6613, time: 130.59113192558289\n",
            "\n",
            "........\n",
            " --- Epoch: 25, train loss: 0.6076, train acc: 0.7057, time: 134.6063826084137\n",
            "Epoch: 25, test loss: 0.6519, test acc: 0.6613, time: 135.23123407363892\n",
            "\n",
            "........\n",
            " --- Epoch: 26, train loss: 0.6050, train acc: 0.7083, time: 140.3116524219513\n",
            "Epoch: 26, test loss: 0.6521, test acc: 0.6612, time: 141.12898230552673\n",
            "\n",
            "........\n",
            " --- Epoch: 27, train loss: 0.6069, train acc: 0.7064, time: 145.2150857448578\n",
            "Epoch: 27, test loss: 0.6522, test acc: 0.6611, time: 145.8228280544281\n",
            "\n",
            "........\n",
            " --- Epoch: 28, train loss: 0.6069, train acc: 0.7064, time: 149.8933298587799\n",
            "Epoch: 28, test loss: 0.6521, test acc: 0.6612, time: 150.51120114326477\n",
            "\n",
            "........\n",
            " --- Epoch: 29, train loss: 0.6059, train acc: 0.7073, time: 155.92063188552856\n",
            "Epoch: 29, test loss: 0.6523, test acc: 0.6610, time: 156.55840492248535\n",
            "\n",
            "........\n",
            " --- Epoch: 30, train loss: 0.6045, train acc: 0.7088, time: 160.6103539466858\n",
            "Epoch: 30, test loss: 0.6518, test acc: 0.6615, time: 161.24972677230835\n",
            "\n",
            "........\n",
            " --- Epoch: 31, train loss: 0.6062, train acc: 0.7071, time: 165.3266158103943\n",
            "Epoch: 31, test loss: 0.6518, test acc: 0.6615, time: 165.93250107765198\n",
            "\n",
            "........\n",
            " --- Epoch: 32, train loss: 0.6064, train acc: 0.7069, time: 171.35850548744202\n",
            "Epoch: 32, test loss: 0.6519, test acc: 0.6613, time: 171.9690546989441\n",
            "\n",
            "........\n",
            " --- Epoch: 33, train loss: 0.6069, train acc: 0.7064, time: 176.0154983997345\n",
            "Epoch: 33, test loss: 0.6509, test acc: 0.6623, time: 176.62574696540833\n",
            "\n",
            "........\n",
            " --- Epoch: 34, train loss: 0.6064, train acc: 0.7069, time: 180.72949504852295\n",
            "Epoch: 34, test loss: 0.6522, test acc: 0.6611, time: 181.52928256988525\n",
            "\n",
            "........\n",
            " --- Epoch: 35, train loss: 0.6052, train acc: 0.7081, time: 186.650235414505\n",
            "Epoch: 35, test loss: 0.6521, test acc: 0.6612, time: 187.28921127319336\n",
            "\n",
            "........\n",
            " --- Epoch: 36, train loss: 0.6066, train acc: 0.7066, time: 191.53273582458496\n",
            "Epoch: 36, test loss: 0.6517, test acc: 0.6616, time: 192.3510820865631\n",
            "\n",
            "........\n",
            " --- Epoch: 37, train loss: 0.6059, train acc: 0.7073, time: 198.11178040504456\n",
            "Epoch: 37, test loss: 0.6518, test acc: 0.6615, time: 198.93593835830688\n",
            "\n",
            "........\n",
            " --- Epoch: 38, train loss: 0.6054, train acc: 0.7078, time: 203.22274136543274\n",
            "Epoch: 38, test loss: 0.6521, test acc: 0.6612, time: 203.83512926101685\n",
            "\n",
            "........\n",
            " --- Epoch: 39, train loss: 0.6059, train acc: 0.7073, time: 207.83328199386597\n",
            "Epoch: 39, test loss: 0.6523, test acc: 0.6610, time: 208.4475495815277\n",
            "\n",
            "........\n",
            " --- Epoch: 40, train loss: 0.6069, train acc: 0.7064, time: 213.58917689323425\n",
            "Epoch: 40, test loss: 0.6519, test acc: 0.6613, time: 214.42314529418945\n",
            "\n",
            "........\n",
            " --- Epoch: 41, train loss: 0.6062, train acc: 0.7071, time: 218.5053412914276\n",
            "Epoch: 41, test loss: 0.6523, test acc: 0.6610, time: 219.11654782295227\n",
            "\n",
            "........\n",
            " --- Epoch: 42, train loss: 0.6059, train acc: 0.7073, time: 223.15430974960327\n",
            "Epoch: 42, test loss: 0.6516, test acc: 0.6617, time: 223.7850661277771\n",
            "\n",
            "........\n",
            " --- Epoch: 43, train loss: 0.6057, train acc: 0.7076, time: 229.19453740119934\n",
            "Epoch: 43, test loss: 0.6524, test acc: 0.6608, time: 229.8111128807068\n",
            "\n",
            "........\n",
            " --- Epoch: 44, train loss: 0.6050, train acc: 0.7083, time: 233.84329962730408\n",
            "Epoch: 44, test loss: 0.6523, test acc: 0.6610, time: 234.46235990524292\n",
            "\n",
            "........\n",
            " --- Epoch: 45, train loss: 0.6069, train acc: 0.7064, time: 238.55156922340393\n",
            "Epoch: 45, test loss: 0.6519, test acc: 0.6613, time: 239.19612860679626\n",
            "\n",
            "........\n",
            " --- Epoch: 46, train loss: 0.6062, train acc: 0.7071, time: 244.63301849365234\n",
            "Epoch: 46, test loss: 0.6523, test acc: 0.6610, time: 245.2874436378479\n",
            "\n",
            "........\n",
            " --- Epoch: 47, train loss: 0.6064, train acc: 0.7069, time: 249.33588862419128\n",
            "Epoch: 47, test loss: 0.6521, test acc: 0.6612, time: 249.94112730026245\n",
            "\n",
            "........\n",
            " --- Epoch: 48, train loss: 0.6066, train acc: 0.7066, time: 254.01308631896973\n",
            "Epoch: 48, test loss: 0.6513, test acc: 0.6620, time: 254.82712030410767\n",
            "\n",
            "........\n",
            " --- Epoch: 49, train loss: 0.6057, train acc: 0.7076, time: 259.9409556388855\n",
            "Epoch: 49, test loss: 0.6523, test acc: 0.6610, time: 260.56950283050537\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNAttention, self).__init__()\n",
        "\n",
        "        # convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # max pooling layers\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # attention layer\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(262144, 128),\n",
        "           # nn.Linear(256 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(256 * 7 * 7, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.max_pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # attention layer\n",
        "        attention_map = self.attention(x.view(x.size(0), -1))\n",
        "        attention_map = attention_map.view(x.size(0), 1, 7, 7)\n",
        "        x = attention_map * x\n",
        "\n",
        "        # fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qpOhsXbgcjIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNNAttention(nn.Module):\n",
        "    def __init__(self, num_classes, attention_dim=64):\n",
        "        super(CNNAttention, self).__init__()\n",
        "        self.conv1 = ConvBlock(1, 64)\n",
        "        self.conv2 = ConvBlock(64, 128)\n",
        "        self.conv3 = ConvBlock(128, 256)\n",
        "        self.attention = nn.Linear(256, attention_dim)\n",
        "        self.fc = nn.Linear(256 * attention_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        # Apply attention\n",
        "        batch_size, num_channels, height, width = out.size()\n",
        "        out = out.view(batch_size, num_channels, -1).mean(dim=2)\n",
        "        attention_weights = F.softmax(self.attention(out), dim=1)\n",
        "        attention_out = attention_weights.unsqueeze(2).expand_as(out) * out\n",
        "        attention_out = attention_out.view(batch_size, -1)\n",
        "\n",
        "        # Final classification\n",
        "        out = self.fc(attention_out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Usage example\n",
        "# model = CNNWithAttention(num_classes=2)\n",
        "# input_tensor = torch.randn(16, 1, 512, 512)  # Example input tensor of shape (batch_size, channels, height, width)\n",
        "# output = model(input_tensor)\n",
        "# print(output.shape)  # (16, 2) - example output shape for binary classification\n"
      ],
      "metadata": {
        "id": "USYBUEchdtKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNAttention(nn.Module):\n",
        "    def __init__(self, num_classes, attention_dim=64):\n",
        "        super(CNNAttention, self).__init__()\n",
        "        self.conv1 = ConvBlock(1, 64)\n",
        "        self.conv2 = ConvBlock(64, 128)\n",
        "        self.conv3 = ConvBlock(128, 256)\n",
        "        self.attention = nn.Linear(256, attention_dim)\n",
        "        self.fc = nn.Linear(256 * attention_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        # Apply attention\n",
        "        batch_size, num_channels, height, width = out.size()\n",
        "        out = out.view(batch_size, num_channels, -1).mean(dim=2)\n",
        "        attention_weights = F.softmax(self.attention(out), dim=1)\n",
        "        attention_weights = attention_weights.unsqueeze(2).unsqueeze(3)\n",
        "        attention_out = attention_weights.expand_as(out) * out\n",
        "        attention_out = attention_out.view(batch_size, -1)\n",
        "\n",
        "        # Final classification\n",
        "        out = self.fc(attention_out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "n-JmI79UfHpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNAttention(nn.Module):\n",
        "    def __init__(self, num_classes, attention_dim=64):\n",
        "        super(CNNAttention, self).__init__()\n",
        "        self.conv1 = ConvBlock(1, 64)\n",
        "        self.conv2 = ConvBlock(64, 128)\n",
        "        self.conv3 = ConvBlock(128, 256)\n",
        "        self.attention = nn.Linear(256, attention_dim)\n",
        "        self.fc = nn.Linear(256 * attention_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        # Apply attention\n",
        "        batch_size, num_channels, height, width = out.size()\n",
        "        out = out.view(batch_size, num_channels, -1).mean(dim=2)\n",
        "        attention_weights = F.softmax(self.attention(out), dim=1)\n",
        "        attention_weights = attention_weights.unsqueeze(2).unsqueeze(3)\n",
        "        attention_out = attention_weights.expand(batch_size, attention_weights.size(1), height, width) * out\n",
        "        attention_out = attention_out.view(batch_size, -1)\n",
        "\n",
        "        # Final classification\n",
        "        out = self.fc(attention_out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "moBaYkJ-fwfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNAttention(1)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "B3PWHWB_ct-n",
        "outputId": "f96179bb-1e7f-44a2-9fe5-bae8b99beede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-984b9054808b>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e1b4b2aef06f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 128, 128] to have 3 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN+Vision transformer"
      ],
      "metadata": {
        "id": "pXVBcSkhoTQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Transformer\n",
        "import torch.nn as nn\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 14400, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=512,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=6,\n",
        "            num_decoder_layers=6,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        X = F.relu(self.conv1(x))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        # X = F.relu(self.conv2(X))\n",
        "        # X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = X.view(X.shape[0], -1)\n",
        "        X = F.relu(self.fc1(X))\n",
        "         #print(X.shape)\n",
        "        X = self.transformer(X,X)\n",
        "        X = F.relu(self.fc2(X))\n",
        "\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "9R5Xd54N50g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet()\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        preds = model(X.float())\n",
        "        loss = loss_fn(preds.squeeze(1), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            test_loss = loss_fn(test_preds, test_y)\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXA1lB_d6X6Y",
        "outputId": "68a43207-6b57-43f2-862e-e318c752e6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 0.6973, train acc: 0.5865, time: 14.244994878768921\n",
            "Epoch: 0, test loss: 0.6317, test acc: 0.7005, time: 16.226632356643677\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 0.6265, train acc: 0.6954, time: 27.537384510040283\n",
            "Epoch: 1, test loss: 0.6175, test acc: 0.7004, time: 28.9745512008667\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 0.6209, train acc: 0.6971, time: 40.17927312850952\n",
            "Epoch: 2, test loss: 0.6110, test acc: 0.7015, time: 41.43684434890747\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 0.6144, train acc: 0.6981, time: 52.5534348487854\n",
            "Epoch: 3, test loss: 0.6106, test acc: 0.7004, time: 53.528202295303345\n",
            "\n",
            "........\n",
            " --- Epoch: 4, train loss: 0.6157, train acc: 0.6976, time: 64.53833246231079\n",
            "Epoch: 4, test loss: 0.6125, test acc: 0.7008, time: 65.46802639961243\n",
            "\n",
            "........\n",
            " --- Epoch: 5, train loss: 0.6145, train acc: 0.6964, time: 78.18585443496704\n",
            "Epoch: 5, test loss: 0.6112, test acc: 0.7009, time: 79.1193950176239\n",
            "\n",
            "........\n",
            " --- Epoch: 6, train loss: 0.6146, train acc: 0.6964, time: 89.61308574676514\n",
            "Epoch: 6, test loss: 0.6097, test acc: 0.7011, time: 91.00420022010803\n",
            "\n",
            "........\n",
            " --- Epoch: 7, train loss: 0.6137, train acc: 0.6969, time: 101.09702134132385\n",
            "Epoch: 7, test loss: 0.6122, test acc: 0.7005, time: 102.24864149093628\n",
            "\n",
            "........\n",
            " --- Epoch: 8, train loss: 0.6139, train acc: 0.6966, time: 113.2812979221344\n",
            "Epoch: 8, test loss: 0.6086, test acc: 0.7013, time: 114.24428176879883\n",
            "\n",
            "........\n",
            " --- Epoch: 9, train loss: 0.6137, train acc: 0.6978, time: 125.46168160438538\n",
            "Epoch: 9, test loss: 0.6106, test acc: 0.7009, time: 126.43384861946106\n",
            "\n",
            "........\n",
            " --- Epoch: 10, train loss: 0.6151, train acc: 0.6961, time: 137.63294124603271\n",
            "Epoch: 10, test loss: 0.6109, test acc: 0.7008, time: 138.58990716934204\n",
            "\n",
            "........\n",
            " --- Epoch: 11, train loss: 0.6154, train acc: 0.6959, time: 149.73753094673157\n",
            "Epoch: 11, test loss: 0.6105, test acc: 0.7004, time: 150.70913910865784\n",
            "\n",
            "........\n",
            " --- Epoch: 12, train loss: 0.6167, train acc: 0.6961, time: 162.48553347587585\n",
            "Epoch: 12, test loss: 0.6110, test acc: 0.7010, time: 163.74517273902893\n",
            "\n",
            "........\n",
            " --- Epoch: 13, train loss: 0.6127, train acc: 0.6981, time: 173.478009223938\n",
            "Epoch: 13, test loss: 0.6125, test acc: 0.7001, time: 174.4549789428711\n",
            "\n",
            "........\n",
            " --- Epoch: 14, train loss: 0.6155, train acc: 0.6959, time: 185.2852292060852\n",
            "Epoch: 14, test loss: 0.6115, test acc: 0.7010, time: 186.2066662311554\n",
            "\n",
            "........\n",
            " --- Epoch: 15, train loss: 0.6188, train acc: 0.6940, time: 197.11472368240356\n",
            "Epoch: 15, test loss: 0.6087, test acc: 0.7013, time: 198.030428647995\n",
            "\n",
            "........\n",
            " --- Epoch: 16, train loss: 0.6225, train acc: 0.6964, time: 208.79200744628906\n",
            "Epoch: 16, test loss: 0.6158, test acc: 0.7006, time: 209.71475768089294\n",
            "\n",
            "........\n",
            " --- Epoch: 17, train loss: 0.6159, train acc: 0.6959, time: 219.80630898475647\n",
            "Epoch: 17, test loss: 0.6121, test acc: 0.7005, time: 221.1167175769806\n",
            "\n",
            "........\n",
            " --- Epoch: 18, train loss: 0.6161, train acc: 0.6969, time: 231.08102345466614\n",
            "Epoch: 18, test loss: 0.6084, test acc: 0.7008, time: 232.00591707229614\n",
            "\n",
            "........\n",
            " --- Epoch: 19, train loss: 0.6134, train acc: 0.6973, time: 244.3459038734436\n",
            "Epoch: 19, test loss: 0.6128, test acc: 0.7011, time: 245.2709677219391\n",
            "\n",
            "........\n",
            " --- Epoch: 20, train loss: 0.6133, train acc: 0.6976, time: 256.0401859283447\n",
            "Epoch: 20, test loss: 0.6096, test acc: 0.7009, time: 256.9658350944519\n",
            "\n",
            "........\n",
            " --- Epoch: 21, train loss: 0.6165, train acc: 0.6952, time: 266.88988995552063\n",
            "Epoch: 21, test loss: 0.6108, test acc: 0.7004, time: 268.17222452163696\n",
            "\n",
            "........\n",
            " --- Epoch: 22, train loss: 0.6124, train acc: 0.6978, time: 278.37395763397217\n",
            "Epoch: 22, test loss: 0.6093, test acc: 0.7006, time: 279.29448413848877\n",
            "\n",
            "........\n",
            " --- Epoch: 23, train loss: 0.6144, train acc: 0.6971, time: 290.2291204929352\n",
            "Epoch: 23, test loss: 0.6095, test acc: 0.7010, time: 291.14680004119873\n",
            "\n",
            "........\n",
            " --- Epoch: 24, train loss: 0.6134, train acc: 0.6971, time: 302.10808062553406\n",
            "Epoch: 24, test loss: 0.6104, test acc: 0.7003, time: 303.0388512611389\n",
            "\n",
            "........\n",
            " --- Epoch: 25, train loss: 0.6137, train acc: 0.6969, time: 314.2393710613251\n",
            "Epoch: 25, test loss: 0.6111, test acc: 0.7004, time: 315.1818468570709\n",
            "\n",
            "........\n",
            " --- Epoch: 26, train loss: 0.6128, train acc: 0.6976, time: 325.3873484134674\n",
            "Epoch: 26, test loss: 0.6105, test acc: 0.7006, time: 326.67525458335876\n",
            "\n",
            "........\n",
            " --- Epoch: 27, train loss: 0.6133, train acc: 0.6971, time: 336.82342767715454\n",
            "Epoch: 27, test loss: 0.6105, test acc: 0.7005, time: 337.76594614982605\n",
            "\n",
            "........\n",
            " --- Epoch: 28, train loss: 0.6140, train acc: 0.6971, time: 348.7454435825348\n",
            "Epoch: 28, test loss: 0.6107, test acc: 0.7008, time: 349.6890411376953\n",
            "\n",
            "........\n",
            " --- Epoch: 29, train loss: 0.6137, train acc: 0.6973, time: 360.6552736759186\n",
            "Epoch: 29, test loss: 0.6101, test acc: 0.7004, time: 361.5782253742218\n",
            "\n",
            "........\n",
            " --- Epoch: 30, train loss: 0.6137, train acc: 0.6976, time: 372.4230797290802\n",
            "Epoch: 30, test loss: 0.6093, test acc: 0.7009, time: 373.3461711406708\n",
            "\n",
            "........\n",
            " --- Epoch: 31, train loss: 0.6130, train acc: 0.6983, time: 383.38484954833984\n",
            "Epoch: 31, test loss: 0.6121, test acc: 0.7004, time: 384.6639213562012\n",
            "\n",
            "........\n",
            " --- Epoch: 32, train loss: 0.6199, train acc: 0.6949, time: 394.77220606803894\n",
            "Epoch: 32, test loss: 0.6111, test acc: 0.7004, time: 395.69862031936646\n",
            "\n",
            "........\n",
            " --- Epoch: 33, train loss: 0.6151, train acc: 0.6981, time: 409.27458667755127\n",
            "Epoch: 33, test loss: 0.6126, test acc: 0.7011, time: 410.22403597831726\n",
            "\n",
            "........\n",
            " --- Epoch: 34, train loss: 0.6151, train acc: 0.6976, time: 421.22660970687866\n",
            "Epoch: 34, test loss: 0.6086, test acc: 0.7009, time: 422.1926944255829\n",
            "\n",
            "........\n",
            " --- Epoch: 35, train loss: 0.6143, train acc: 0.6978, time: 433.3490204811096\n",
            "Epoch: 35, test loss: 0.6100, test acc: 0.7009, time: 434.27588295936584\n",
            "\n",
            "........\n",
            " --- Epoch: 36, train loss: 0.6132, train acc: 0.6978, time: 444.75081276893616\n",
            "Epoch: 36, test loss: 0.6110, test acc: 0.7013, time: 446.0492651462555\n",
            "\n",
            "........\n",
            " --- Epoch: 37, train loss: 0.6142, train acc: 0.6976, time: 457.45053720474243\n",
            "Epoch: 37, test loss: 0.6101, test acc: 0.7004, time: 458.76218008995056\n",
            "\n",
            "........\n",
            " --- Epoch: 38, train loss: 0.6153, train acc: 0.6957, time: 469.5763318538666\n",
            "Epoch: 38, test loss: 0.6117, test acc: 0.7008, time: 470.709361076355\n",
            "\n",
            "........\n",
            " --- Epoch: 39, train loss: 0.6138, train acc: 0.6959, time: 485.5953531265259\n",
            "Epoch: 39, test loss: 0.6106, test acc: 0.7003, time: 487.09643363952637\n",
            "\n",
            "........\n",
            " --- Epoch: 40, train loss: 0.6157, train acc: 0.6969, time: 498.5265591144562\n",
            "Epoch: 40, test loss: 0.6110, test acc: 0.7001, time: 499.5285189151764\n",
            "\n",
            "........\n",
            " --- Epoch: 41, train loss: 0.6153, train acc: 0.6961, time: 510.27835869789124\n",
            "Epoch: 41, test loss: 0.6127, test acc: 0.7003, time: 511.21243691444397\n",
            "\n",
            "....."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNVisionTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNVisionTransformer, self).__init__()\n",
        "\n",
        "        # Convert 1 channel to 3 channels\n",
        "        self.conv1 = nn.Conv2d(1, 3, kernel_size=1)\n",
        "\n",
        "        # CNN backbone\n",
        "        self.cnn = resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()\n",
        "\n",
        "        # Vision Transformer\n",
        "        self.transformer = Transformer(\n",
        "            d_model=512,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=6,\n",
        "            num_decoder_layers=6,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(512, 1)\n",
        "        self.sigmoid = nn.sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert 1 channel to 3 channels\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # CNN backbone\n",
        "        cnn_features = self.cnn(x)\n",
        "\n",
        "        # Vision Transformer\n",
        "        transformer_output = self.transformer(cnn_features, cnn_features)  # Pass same input as source and target\n",
        "\n",
        "        # Apply global average pooling\n",
        "        # pooled_features = F.adaptive_avg_pool2d(transformer_output, 1)\n",
        "        # pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.fc(transformer_output)  # Apply sigmoid activation and squeeze the output\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "-58Yx2o8vwAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = CNNAttention(1)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "epoches = 50\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Model Training...\n",
        "for epoch in range(epoches):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "        # print(X.shape)\n",
        "        preds = model(X.float())\n",
        "        # print(preds.size())\n",
        "        # print(preds)\n",
        "        loss = loss_fn(preds.squeeze(1).float(), y.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
        "        epoch_accuracy += accuracy\n",
        "        epoch_loss += loss\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    epoch_accuracy = epoch_accuracy/len(trainloader)\n",
        "    accuracies.append(epoch_accuracy)\n",
        "    epoch_loss = epoch_loss / len(trainloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "\n",
        "    # test set accuracy\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_accuracy = 0\n",
        "\n",
        "        for test_X, test_y in testloader:\n",
        "\n",
        "            test_preds = model(test_X.float())\n",
        "            #print(test_preds.shape, test_y.shape)\n",
        "            test_loss = loss_fn(test_preds.squeeze().float(), test_y.float())\n",
        "\n",
        "            test_epoch_loss += test_loss\n",
        "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "            test_epoch_accuracy += test_accuracy\n",
        "\n",
        "        test_epoch_accuracy = test_epoch_accuracy/len(testloader)\n",
        "        test_epoch_loss = test_epoch_loss / len(testloader)\n",
        "\n",
        "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "O7R3na1AoWkX",
        "outputId": "810be751-3eba-4d22-f1d3-c2a04280c918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........\n",
            " --- Epoch: 0, train loss: 185.9907, train acc: 0.2856, time: 63.23578071594238\n",
            "Epoch: 0, test loss: 165.5942, test acc: 0.3702, time: 67.52071404457092\n",
            "\n",
            "........\n",
            " --- Epoch: 1, train loss: 185.7078, train acc: 0.2848, time: 123.21629071235657\n",
            "Epoch: 1, test loss: 165.1619, test acc: 0.3702, time: 127.55704617500305\n",
            "\n",
            "........\n",
            " --- Epoch: 2, train loss: 185.6349, train acc: 0.2851, time: 186.6211929321289\n",
            "Epoch: 2, test loss: 164.7935, test acc: 0.3694, time: 191.8702073097229\n",
            "\n",
            "........\n",
            " --- Epoch: 3, train loss: 185.7292, train acc: 0.2856, time: 248.0689995288849\n",
            "Epoch: 3, test loss: 165.2690, test acc: 0.3709, time: 255.41748523712158\n",
            "\n",
            "......"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-dabd6f81d4ca>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEKOgK_KvJl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uJMJa53NYqSW",
        "bh86br1q_G-v",
        "ImzIE7Ov7c76",
        "fn3IhWbhS0B8",
        "RyUp2uqkTVFv"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}